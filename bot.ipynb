{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "847f8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import TextLoader,PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "583de525",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"document01.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10730db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0654814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80a306e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "print(chunks.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec0f9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b57c0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    texts=[doc.page_content for doc in chunks],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f369c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa9dc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"[GROQ_API_KEY]\"\n",
    "llm_compression = ChatGroq(temperature=0,api_key=GROQ_API_KEY,model=\"llama-3.3-70b-versatile\")\n",
    "compressor_llm = LLMChainExtractor.from_llm(llm_compression)\n",
    "advanced_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor_llm,\n",
    "    base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a3669af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query):\n",
    "    \"\"\"Récupère le contexte pertinent pour la question\"\"\"\n",
    "    docs = advanced_retriever.get_relevant_documents(query)\n",
    "    return \"\\n\\n\".join([f\"- {doc.page_content}\" for doc in docs])\n",
    "\n",
    "# --- Configuration du prompt RAG ---\n",
    "rag_template = \"\"\"**Rôle** : Expert in analysing repport .\n",
    "**Mission** : Giving though in how to improve the repport.\n",
    "\n",
    "**Historique** :\n",
    "{history}\n",
    "\n",
    "**Connaissances** :\n",
    "{context}\n",
    "\n",
    "**Question** :\n",
    "{question}\n",
    "\n",
    "**Consignes** :\n",
    "- Répondez uniquement avec les informations disponibles\n",
    "- En cas de hors-sujet : \"Je n'ai pas cette information. Voulez-vous une question sur les tendances énergétiques marocaines ?\"\n",
    "- Ton professionnel et clair\n",
    "\n",
    "**Format** :\n",
    "Structure claire avec informations factuelles\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "\n",
    "llm_response = ChatGroq(temperature=0,model=\"llama-3.3-70b-versatile\",api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=False)\n",
    "\n",
    "def generate_response(user_message):\n",
    "    \"\"\"Génère la réponse de l'assistant\"\"\"\n",
    "\n",
    "    conversation_history = memory.buffer\n",
    "\n",
    "\n",
    "    context = retrieve_context(user_message)\n",
    "\n",
    "\n",
    "    prompt = rag_prompt.format(\n",
    "        history=conversation_history,\n",
    "        context=context,\n",
    "        question=user_message\n",
    "    )\n",
    "\n",
    "\n",
    "    response = llm_response([HumanMessage(content=prompt)]).content\n",
    "\n",
    "\n",
    "    memory.save_context({\"input\": user_message}, {\"output\": response})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e7d1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a40c0fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83920/1037708201.py:3: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Djnago \")\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt_input = gr.Textbox(\n",
    "            show_label=False,\n",
    "            placeholder=\"Posez votre question ici...\",\n",
    "            container=False\n",
    "        )\n",
    "        send_btn = gr.Button(\"Envoyer\")\n",
    "\n",
    "    def user_interaction(user_message, history):\n",
    "        response = generate_response(user_message)\n",
    "        history.append((user_message, response))\n",
    "        return \"\", history\n",
    "\n",
    "    txt_input.submit(user_interaction, [txt_input, state], [txt_input, chatbot])\n",
    "    send_btn.click(user_interaction, [txt_input, state], [txt_input, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bfac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1252b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
